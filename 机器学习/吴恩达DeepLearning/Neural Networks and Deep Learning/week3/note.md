# NN Representation

![1572487824128](pics/1572487824128.png)

我们有输入特征$x_1$、$x_2$、$x_3$，它们被竖直地堆叠起来，这叫做神经网络的**输入层**。它包含了神经网络的输入；然后这里有另外一层我们称之为**隐藏层**。图中的最后一层只由一个结点构成，而这个只有一个结点的层被称为**输出层**，它负责产生预测值。

隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，你是无法看到的。 

![1573710301084](pics/1573710301084.png)

我们约定input layer不算做一个层，所以上图是一个两层的网络。hidden layer记作$a^{[1]}$，outputlayer记作$a^{[2]}$，$a$代表激活的意思，它意味着网络中不同层的值会传递到他们后面的层中，$a^{[1]}$中有三个节点，我们把第一个节点记作$a^{[1]}_1$，剩下的类推。
$$
a^{[1]}=\begin{bmatrix}
a^{[1]}_1\\ 
a^{[2]}_2\\ 
a^{[3]}_3\\ 
\end{bmatrix}
$$

$\bold w,\bold b$是$(4,3)$和$(4,1)$维的向量：

​	注意，$\bold w$是转置后的向量。

- $\bold w$的4代表4个单元，3代表3个对应着特征$x$的权重变量，即$\bold w_1^{[1]}$为$(1,3)$的向量；

- $\bold b$的4对应着4个单元中b的值。


# NN‘s Output Computing

神经网络的计算，以LR为例，如下图所示：用**圆圈**表示神经网络的计算单元， 逻辑回归的计算有两个步骤，首先你按步骤计算出$z$，然后在第二步中你以**sigmoid**函数为激活函数计算$a$，一个神经网络只是这样子做了好多次重复计算。 计算出每个神经元的值，然后传播给下一层计算。式子略。

![1572490407636](pics/1572490407636.png)



**向量化计算**上述步骤， 向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，每个向量$\bold w$对应着一个单元，把这些向量堆积起来，就会得到$\bold w$的矩阵，用$w^{[n]}$表示某一层的神经网络。

先计算$z$：
$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$
之后带入到sigmoid：
$$
a^{[1]} =
	\left[
		\begin{array}{c}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{array}
		\right]
		= \sigma(z^{[1]})
$$
隐藏层$a^{[1]}$完成，接下来输出层也是一样的：
$$
Z^{[2]}=w^{[2]}a^{[1]}+b^{[2]} 
\\
a^{[[2]]}=\sigma(z^{[2]})
$$

![1573713915170](pics/1573713915170.png)