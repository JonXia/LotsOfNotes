- 阅读《李航统计学习方法》的65-74页
  - 学习Gini指数
  - 学习回归树
  - 剪枝

1. CART算法

  1984年提出，1和2由此引入，CART算法同样由特征选择、树生成、剪枝组成，既可用来分类也可用于回归。



2. 剪枝
 - 剪枝：决策树过拟合风险很大，理论上可以完全分得开数据
 - 策略：
    - 预剪枝:一边建立树一边进行剪枝操作
    - 后剪枝:当建立完决策树后进行剪枝操作
 - 预剪枝：限制深度，叶子节点个数，叶子节点样本数，
 - 后剪枝：通过一定标准衡量
 决策树的剪枝往往通过极小化决策树整体的损失函数来实现，设树T的叶结点个数为|T|，t是树T的叶结点，该叶结点有$N_{t}$个样本点，其中k类的样本点有N_{tk}个，k=1,2,...,K，H_{t}(T)为叶结点t上的经验熵，$\alpha\geq 0$为参数，则决策树学习的损失函数可以定义为。
 $
C_{a}(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}(T)+\alpha |T|
 $

$\sum_{t=1}^{\left | T \right |}N_{t}H_{t}(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数$\alpha\geq0$控制两者之间的影响。
