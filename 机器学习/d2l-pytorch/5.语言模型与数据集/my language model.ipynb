{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语言模型\n",
    "一段自然语言文本可以看作是一个离散的时间序列，给定一个长度为$T$的词的序列，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：\n",
    "$$\n",
    "P(w_1,w_2,...,w_T)\n",
    "$$\n",
    "\n",
    "本章主要介绍**基于统计的语言模型**，主要是元语法(-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语言模型\n",
    "假设序列$w_1,w_2,...,w_T$中的每个词是一次生成的，我们有\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_1, w_2, \\ldots, w_T)\n",
    "&= \\prod_{t=1}^T P(w_t \\mid w_1, \\ldots, w_{t-1})\\\\\n",
    "&= P(w_1)P(w_2 \\mid w_1) \\cdots P(w_T \\mid w_1w_2\\cdots w_{T-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "例如，一段含有4个词的文本序列的概率：\n",
    "$$\n",
    "P(w_1,w_2,w_3,w_4)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)P(w_4|w_1,w_2,w_3)\n",
    "$$\n",
    "语言模型的参数就是词的概率一级给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，入维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：\n",
    "\n",
    "$$\n",
    "\\hat P(w_1) = \\frac{n(w_1)}{n}\n",
    "$$\n",
    "\n",
    "其中$n(w_1)$为语料库总$w_1$作为第一个词文本的数量，$n$为语料库中文本的总数量。\n",
    "\n",
    "类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：\n",
    "\n",
    "$$\n",
    "\\hat P(w_2 \\mid w_1) = \\frac{n(w_1, w_2)}{n(w_1)}\n",
    "$$\n",
    "\n",
    "其中$n(w_1,w_2)$语料库中以$w_1$作为第一个词，$w_2$第二个词的文本的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n元语法\n",
    "序列长度增加，计算和存储多个词共同出现的概率的复杂度会成指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔可夫假设是指一个词的出现只跟前面n个词相关，即$n$阶马尔科夫链(Markov chain of order $n$)，如果$n=1$，那么有$P(w_3|w_1,w_2)=P(w_3|w_2)$。基于$n-1$阶马尔科夫链，我们可以将语言模型改写为\n",
    "$$\n",
    "P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1})\n",
    "$$\n",
    "以上也叫n元语法，它是基于$n-1$阶马尔科夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(w_1, w_2, w_3, w_4)\n",
    "&= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3)\\\\\n",
    "&= P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "当$n$分别为1、2和3时，我们将其分别称作一元语法(unigram)、二元语法(bigram)和三元语法(trigram)。例如，长度为4的序列$w_1,w_2,w_3,w_4$在一元语法、二元语法和三元语法的概率为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\\\\n",
    "P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\\\\n",
    "P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "当$n$较小时，$n$元语法往往不准确。例如，在一元语法中，由三个词组的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
