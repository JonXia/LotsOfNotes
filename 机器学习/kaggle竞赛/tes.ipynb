{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置根目录\n",
    "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR = ROOT / \"parquet_files\" / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    return process_dataframe(df, depth)\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = [process_dataframe(pl.read_parquet(path), depth) for path in glob.glob(str(regex_path))]\n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "def process_dataframe(df, depth=None):\n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df):\n",
    "        dtype_mapping = {\n",
    "            \"case_id\": pl.Int32,\n",
    "            \"WEEK_NUM\": pl.Int32,\n",
    "            \"num_group1\": pl.Int32,\n",
    "            \"num_group2\": pl.Int32,\n",
    "            \"date_decision\": pl.Date,\n",
    "        }\n",
    "        for col in df.columns:\n",
    "            if col in dtype_mapping:\n",
    "                df = df.with_columns(pl.col(col).cast(dtype_mapping[col]))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] == \"M\":\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] == \"D\":\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        date_cols = [col for col in df.columns if col[-1] == \"D\"]\n",
    "        for col in date_cols:\n",
    "            df = df.with_columns(\n",
    "                (pl.col(col) - pl.col(\"date_decision\")).alias(col)\n",
    "            ).with_columns(\n",
    "                pl.col(col).dt.total_days().cast(pl.Float32).alias(col)\n",
    "            )\n",
    "        df = df.drop([\"date_decision\", \"MONTH\"])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_cols(df):\n",
    "        null_ratio = df.select([pl.col(col).is_null().mean().alias(col) for col in df.columns])\n",
    "        high_null_cols = [col for col in null_ratio.columns if null_ratio[col][0] > 0.95 and col not in [\"target\", \"case_id\", \"WEEK_NUM\"]]\n",
    "        df = df.drop(high_null_cols)\n",
    "\n",
    "        string_cols = [col for col in df.columns if df[col].dtype == pl.String]\n",
    "        for col in string_cols:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                freq = df[col].n_unique()\n",
    "                if freq == 1 or freq > 200:\n",
    "                    df = df.drop(col)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚合器\n",
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] == \"D\"]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "df_train = feature_eng(**data_store)\n",
    "print(\"Train data shape:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤列\n",
    "df_train = Pipeline.filter_cols(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 Pandas DataFrame\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "df_train, cat_cols = to_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离特征和目标变量\n",
    "X = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "y = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逻辑回归分类\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "# 评估逻辑回归模型\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBDT 分类\n",
    "gbdt = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbdt.fit(X_train, y_train)\n",
    "y_pred_gbdt = gbdt.predict(X_test)\n",
    "\n",
    "# 评估 GBDT 模型\n",
    "print(\"GBDT Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_gbdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 CatBoost 进行分类\n",
    "cat_features = [col for col in X_train.columns if X_train[col].dtype.name in ['object', 'category']]\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,  # 迭代次数\n",
    "    learning_rate=0.03,  # 学习率\n",
    "    depth=6,  # 树的深度\n",
    "    loss_function='Logloss',  # 损失函数\n",
    "    eval_metric='Accuracy',  # 评估指标\n",
    "    random_seed=42,  # 随机种子\n",
    "    verbose=100,  # 每 100 次迭代打印一次信息\n",
    "    task_type='GPU',  # 使用 GPU\n",
    "    devices='0:1'  # 使用哪些 GPU 设备，例如 '0:1' 表示使用第 0 和第 1 号 GPU\n",
    ")\n",
    "\n",
    "# 训练 CatBoost 模型\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    cat_features=cat_features,  # 显式指定分类特征\n",
    "    early_stopping_rounds=100,  # 早停，如果 100 轮内没有提升则停止训练\n",
    "    use_best_model=True,  # 使用最佳模型\n",
    "    plot=True  # 绘制训练过程中的学习曲线\n",
    ")\n",
    "\n",
    "# 预测\n",
    "y_pred_catboost = model.predict(X_test)\n",
    "\n",
    "# 评估 CatBoost 模型\n",
    "print(\"CatBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_catboost))\n",
    "\n",
    "# 保存模型\n",
    "model.save_model('catboost_gpu_model.cbm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}