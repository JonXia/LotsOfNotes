### 信贷风险模型稳定性竞赛总结

### Overview页概述
该竞赛旨在预测客户的违约情况，评估模型不仅基于AUC，还考虑模型在测试集上的预测稳定性。

#### 数据集描述
- 数据集包含多个表格，涵盖内部和外部信息。
- 主要表格类型：
  - **基础表**：存储观察和唯一标识符（case_id）。
  - **静态表**：包含特定客户的静态特征。
  - **历史表**：记录客户的历史信息，分为不同深度。

#### 特征说明
- **特殊列**：
  - `case_id`：每个信贷案例的唯一标识符。
  - `date_decision`：贷款批准决策的日期。
  - `WEEK_NUM`：用于聚合的周数。
  - `target`：客户是否违约的目标值。

- **深度定义**：
  - 深度0：静态特征。
  - 深度1：基于历史记录的特征。
  - 深度2：更复杂的历史记录特征。

#### 数据处理
- 对于深度大于0的表格，需要使用聚合函数将历史记录转化为单个特征。
- 特征转换规则包括不同的标识符，如P（转化DPD）、M（掩码类别）等，简化特征操作。

#### 重要链接
- 详细特征定义可在`feature_definitions.csv`中找到。

### 结论
该竞赛为参与者提供了丰富的数据资源和明确的评价标准，旨在提升信贷风险预测的准确性和稳定性。



预测的是每一笔贷款在WEEK_NUMS的违约概率

提交的结果是AUC，但是训练的时候要以$mean(gini)+88.0*min(0,a)-0.5.std(residuals)$

$gini=2*AUC-1$



### Data页概述

好的，以下是对数据集描述的详细解析：

### 数据集概述

1. **竞赛目标**：
   - 预测客户是否会违约，评估模型的表现不仅依赖于预测的准确性（AUC），还要考虑模型在不同数据范围内的稳定性。

2. **数据来源**：
   - 数据集包含来自多个内部和外部信息源的数据，涉及各种表格。

### 表格类型

- **基础表**：
  - 存储基本信息，包括每个观察的唯一标识符（`case_id`），该ID用于将其他表的数据与基础表进行关联。

- **静态表**：
  - 包含与特定客户相关的静态特征信息。这些特征不会随时间变化。

- **历史表**：
  - 记录客户的历史信息，通常按时间顺序排列，分为不同的深度：
    - **深度0**：包含与特定`case_id`直接相关的特征。
    - **深度1**：每个`case_id`有一个历史记录，通常以`num_group1`索引。
    - **深度2**：更复杂的历史记录，使用`num_group1`和`num_group2`进行索引。

### 特殊列

- **`case_id`**：每个信贷案例的唯一标识符，必须用于关联其他表。
- **`date_decision`**：贷款审批的日期。
- **`WEEK_NUM`**：用于聚合的周数，测试集中的`WEEK_NUM`会在训练集的最后值基础上继续增加。
- MONTH：月份
- **`target`**：表示客户是否违约的目标变量。
- num_group1 - 这是一个用于深度为1和深度为2表中 case_id 历史记录的索引列。 
- num_group2 - 这是深度为2表中的 case_id 历史记录的第二索引列的顺序很重要，并将在特征定义中予以澄清。 num_group1 num_group2 表中的所有其他原始列均用作预测变量。它们的定义可以在文件中找到。 对于深度为0的表，预测变量可以直接用作特征。 但是，对于深度>0的表，您可能需要使用聚合函数，将与每个相关的历史记录整合成单个特征。 在表中，或代表人的索引（在预测变量的定义中很明确）零索引具有特殊含义。 当等于0时，它是申请人（申请贷款的人）。 feature_definitions.csv case_id num_group1 num_group2 num_groupN 
- 各种预测变量已进行转换，因此我们对相似转换组使用以下符号表示 
  - P - 转换 DPD（逾期天数） 
  - M - 遮蔽类别 
  - A - 转换金额 
  - D - 转换日期 
  - T - 未指定转换 
  - L - 未指定转换


### 数据处理

- **特征聚合**：对于深度大于0的表格，通常需要使用聚合函数将历史记录合并成一个特征。例如，可以计算过去几个月的平均值或总和。
- **特征转换**：数据集中有一系列转换规则，用字母标识不同的转换类型，如：
  - **P**：转化DPD（逾期天数）
  - **M**：掩码类别
  - **A**：金额转化
  - **D**：日期转化

## 表格

根据你提供的信息，以下是按照深度（`depth`）分类的表格列表：

### Depth = 0 (静态表，直接与 `case_id` 相关)：
- **Base tables**：
  - `train_base.csv`
  - `test_base.csv`
  
- **static_0**（内部数据源）：
  - `train_static_0_0.csv`
  - `train_static_0_1.csv`
  - `test_static_0_0.csv`
  - `test_static_0_1.csv`
  - `test_static_0_2.csv`
  
- **static_cb_0**（外部数据源）：
  - `train_static_cb_0.csv`
  - `test_static_cb_0.csv`

### Depth = 1 (与 `case_id` 相关的历史记录，使用 `num_group1` 索引)：
- **applprev_1**（内部数据源）：
  - `train_applprev_1_0.csv`
  - `train_applprev_1_1.csv`
  - `test_applprev_1_0.csv`
  - `test_applprev_1_1.csv`
  - `test_applprev_1_2.csv`
  
- **other_1**（内部数据源）：
  - `train_other_1.csv`
  - `test_other_1.csv`
  
- **tax_registry_a_1**（外部数据源，税务注册表A）：
  - `train_tax_registry_a_1.csv`
  - `test_tax_registry_a_1.csv`
  
- **tax_registry_b_1**（外部数据源，税务注册表B）：
  - `train_tax_registry_b_1.csv`
  - `test_tax_registry_b_1.csv`
  
- **tax_registry_c_1**（外部数据源，税务注册表C）：
  - `train_tax_registry_c_1.csv`
  - `test_tax_registry_c_1.csv`
  
- **credit_bureau_a_1**（外部数据源，信用局A）：
  - `train_credit_bureau_a_1_0.csv`
  - `train_credit_bureau_a_1_1.csv`
  - `train_credit_bureau_a_1_2.csv`
  - `train_credit_bureau_a_1_3.csv`
  - `test_credit_bureau_a_1_0.csv`
  - `test_credit_bureau_a_1_1.csv`
  - `test_credit_bureau_a_1_2.csv`
  - `test_credit_bureau_a_1_3.csv`
  - `test_credit_bureau_a_1_4.csv`
  
- **credit_bureau_b_1**（外部数据源，信用局B）：
  - `train_credit_bureau_b_1.csv`
  - `test_credit_bureau_b_1.csv`
  
- **deposit_1**（内部数据源）：
  - `train_deposit_1.csv`
  - `test_deposit_1.csv`
  
- **person_1**（内部数据源）：
  - `train_person_1.csv`
  - `test_person_1.csv`
  
- **debitcard_1**（内部数据源）：
  - `train_debitcard_1.csv`
  - `test_debitcard_1.csv`

### Depth = 2 (与 `case_id` 相关的历史记录，使用 `num_group1` 和 `num_group2` 双重索引)：
- **applprev_2**（内部数据源）：
  - `train_applprev_2.csv`
  - `test_applprev_2.csv`
  
- **person_2**（内部数据源）：
  - `train_person_2.csv`
  - `test_person_2.csv`
  
- **credit_bureau_a_2**（外部数据源，信用局A）：
  - `train_credit_bureau_a_2_0.csv`
  - `train_credit_bureau_a_2_1.csv`
  - `train_credit_bureau_a_2_2.csv`
  - `train_credit_bureau_a_2_3.csv`
  - `train_credit_bureau_a_2_4.csv`
  - `train_credit_bureau_a_2_5.csv`
  - `train_credit_bureau_a_2_6.csv`
  - `train_credit_bureau_a_2_7.csv`
  - `train_credit_bureau_a_2_8.csv`
  - `train_credit_bureau_a_2_9.csv`
  - `train_credit_bureau_a_2_10.csv`
  - `test_credit_bureau_a_2_0.csv`
  - `test_credit_bureau_a_2_1.csv`
  - `test_credit_bureau_a_2_2.csv`
  - `test_credit_bureau_a_2_3.csv`
  - `test_credit_bureau_a_2_4.csv`
  - `test_credit_bureau_a_2_5.csv`
  - `test_credit_bureau_a_2_6.csv`
  - `test_credit_bureau_a_2_7.csv`
  - `test_credit_bureau_a_2_8.csv`
  - `test_credit_bureau_a_2_9.csv`
  - `test_credit_bureau_a_2_10.csv`
  - `test_credit_bureau_a_2_11.csv`
  
- **credit_bureau_b_2**（外部数据源，信用局B）：
  - `train_credit_bureau_b_2.csv`
  - `test_credit_bureau_b_2.csv`

这就是所有表格按深度（depth）进行的分类，表格通过 `case_id` 进行连接，深度越高，表示与 `case_id` 相关的历史记录越复杂，涉及多个索引。

### 结论

这个数据集提供了丰富的特征和历史记录，适合用于机器学习模型的训练和评估。理解这些表格和特征的结构对于构建有效的预测模型至关重要。

#### 大数据集的建议

> 处理大数据集时，可以采取以下策略来提高效率和效果：
>
> ### 1. 数据预处理
> - **数据清洗**：检查缺失值、重复数据和异常值。可以使用插值法或填充方法处理缺失值。
> - **数据类型优化**：将数据类型转换为更节省内存的格式（如将`float64`转换为`float32`，将`int64`转换为`int32`）。
>
> ### 2. 数据采样
> - **随机抽样**：如果数据集过大，可以考虑随机抽取一部分样本进行初步分析和模型训练。
> - **分层抽样**：确保抽样中各类目标变量（如违约与否）的比例与整体数据集一致。
>
> ### 3. 特征选择
> - **降维技术**：使用PCA（主成分分析）或LDA（线性判别分析）等方法减少特征数量。
> - **特征重要性评估**：使用模型（如随机森林）评估特征的重要性，保留重要特征，剔除冗余特征。
>
> ### 4. 数据分块处理
> - **分批处理**：将数据集分为多个小块，逐块加载和处理，避免内存溢出。
> - **使用生成器**：在训练模型时，使用数据生成器逐批加载数据。
>
> ### 5. 并行处理
> - **多线程/多进程**：利用Python的`multiprocessing`库或`joblib`库并行处理数据，提高处理速度。
> - **分布式计算**：使用Apache Spark或Dask等工具进行分布式数据处理，适合处理超大规模数据集。
>
> ### 6. 使用高效的数据存储格式
> - **压缩格式**：使用Parquet或Feather格式存储数据，这些格式支持高效的读写和压缩。
> - **数据库**：将数据存储在数据库中（如PostgreSQL、MongoDB），通过SQL查询高效提取所需数据。
>
> ### 7. 模型选择与调优
> - **选择适合大数据的算法**：如XGBoost、LightGBM等，这些算法在处理大数据时表现较好。
> - **超参数调优**：使用交叉验证和网格搜索等方法优化模型参数，避免过拟合。
>
> ### 8. 监控与评估
> - **实时监控**：设置监控机制，观察数据处理和模型训练的性能，及时调整策略。
> - **模型评估**：使用适当的评估指标（如AUC、F1-score）评估模型性能，确保模型的稳定性和准确性。
>
> 通过上述策略，可以有效处理和分析大数据集，提高机器学习模型的性能和效率。

```py
# We need to use aggregation functions in tables with depth > 1, so tables that contain num_group1 column or 
# also num_group2 column.
train_person_1_feats_1 = train_person_1.group_by("case_id").agg(
    pl.col("mainoccupationinc_384A").max().alias("mainoccupationinc_384A_max"),
    (pl.col("incometype_1044T") == "SELFEMPLOYED").max().alias("mainoccupationinc_384A_any_selfemployed")
)

# Here num_group1=0 has special meaning, it is the person who applied for the loan.
train_person_1_feats_2 = train_person_1.select(["case_id", "num_group1", "housetype_905L"]).filter(
    pl.col("num_group1") == 0
).drop("num_group1").rename({"housetype_905L": "person_housetype"})

# Here we have num_goup1 and num_group2, so we need to aggregate again.
train_credit_bureau_b_2_feats = train_credit_bureau_b_2.group_by("case_id").agg(
    pl.col("pmts_pmtsoverdue_635A").max().alias("pmts_pmtsoverdue_635A_max"),
    (pl.col("pmts_dpdvalue_108P") > 31).max().alias("pmts_dpdvalue_108P_over31")
)

# We will process in this examples only A-type and M-type columns, so we need to select them.
selected_static_cols = []
for col in train_static.columns:
    if col[-1] in ("A", "M"):
        selected_static_cols.append(col)
print(selected_static_cols)

selected_static_cb_cols = []
for col in train_static_cb.columns:
    if col[-1] in ("A", "M"):
        selected_static_cb_cols.append(col)
print(selected_static_cb_cols)

# Join all tables together.
data = train_basetable.join(
    train_static.select(["case_id"]+selected_static_cols), how="left", on="case_id"
).join(
    train_static_cb.select(["case_id"]+selected_static_cb_cols), how="left", on="case_id"
).join(
    train_person_1_feats_1, how="left", on="case_id"
).join(
    train_person_1_feats_2, how="left", on="case_id"
).join(
    train_credit_bureau_b_2_feats, how="left", on="case_id"
)
test_person_1_feats_1 = test_person_1.group_by("case_id").agg(
    pl.col("mainoccupationinc_384A").max().alias("mainoccupationinc_384A_max"),
    (pl.col("incometype_1044T") == "SELFEMPLOYED").max().alias("mainoccupationinc_384A_any_selfemployed")
)

test_person_1_feats_2 = test_person_1.select(["case_id", "num_group1", "housetype_905L"]).filter(
    pl.col("num_group1") == 0
).drop("num_group1").rename({"housetype_905L": "person_housetype"})

test_credit_bureau_b_2_feats = test_credit_bureau_b_2.group_by("case_id").agg(
    pl.col("pmts_pmtsoverdue_635A").max().alias("pmts_pmtsoverdue_635A_max"),
    (pl.col("pmts_dpdvalue_108P") > 31).max().alias("pmts_dpdvalue_108P_over31")
)

data_submission = test_basetable.join(
    test_static.select(["case_id"]+selected_static_cols), how="left", on="case_id"
).join(
    test_static_cb.select(["case_id"]+selected_static_cb_cols), how="left", on="case_id"
).join(
    test_person_1_feats_1, how="left", on="case_id"
).join(
    test_person_1_feats_2, how="left", on="case_id"
).join(
    test_credit_bureau_b_2_feats, how="left", on="case_id"
)
case_ids = data["case_id"].unique().shuffle(seed=1)
case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)
case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)

cols_pred = []
for col in data.columns:
    if col[-1].isupper() and col[:-1].islower():
        cols_pred.append(col)

print(cols_pred)

def from_polars_to_pandas(case_ids: pl.DataFrame) -> pl.DataFrame:
    return (
        data.filter(pl.col("case_id").is_in(case_ids))[["case_id", "WEEK_NUM", "target"]].to_pandas(),
        data.filter(pl.col("case_id").is_in(case_ids))[cols_pred].to_pandas(),
        data.filter(pl.col("case_id").is_in(case_ids))["target"].to_pandas()
    )

base_train, X_train, y_train = from_polars_to_pandas(case_ids_train)
base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid)
base_test, X_test, y_test = from_polars_to_pandas(case_ids_test)

for df in [X_train, X_valid, X_test]:
    df = convert_strings(df)
```



这段代码是使用Polars库处理数据的一个例子，主要目的是为机器学习模型准备训练和测试数据。下面是对每个部分的解释：

### 数据聚合
1. **`train_person_1_feats_1`**:
   - 对`train_person_1`表按`case_id`分组。
   - 计算`mainoccupationinc_384A`列的最大值，并命名为`mainoccupationinc_384A_max`。
   - 判断`incometype_1044T`列是否有任何记录等于"SELFEMPLOYED"，并将布尔结果转换为整数（True -> 1, False -> 0），取最大值作为最终结果，命名为`mainoccupationinc_384A_any_selfemployed`。

2. **`train_person_1_feats_2`**:
   - 选择`case_id`, `num_group1`, 和`housetype_905L`三列。
   - 过滤出`num_group1`等于0的行，这表示贷款申请人。
   - 删除`num_group1`列。
   - 将`housetype_905L`列重命名为`person_housetype`。

3. **`train_credit_bureau_b_2_feats`**:
   - 对`train_credit_bureau_b_2`表按`case_id`分组。
   - 计算`pmts_pmtsoverdue_635A`列的最大值，并命名为`pmts_pmtsoverdue_635A_max`。
   - 判断`pmts_dpdvalue_108P`列是否大于31天，将布尔结果转换为整数并取最大值，命名为`pmts_dpdvalue_108P_over31`。

### 选择特定类型的列
- 从`train_static`和`train_static_cb`中选择以"A"或"M"结尾的列，并分别存储在`selected_static_cols`和`selected_static_cb_cols`列表中。

### 数据连接
- 使用`case_id`作为键，将多个表格进行左连接（left join）到基础表`train_basetable`上，形成一个包含所有相关信息的数据集`data`。

### 测试数据的处理
- 对测试数据执行与训练数据相同的特征工程步骤，得到`test_person_1_feats_1`、`test_person_1_feats_2`、`test_credit_bureau_b_2_feats`，然后将这些特征加入到`test_basetable`中，生成`data_submission`。

### 数据分割
- 获取`case_id`的唯一值并打乱顺序。
- 使用`train_test_split`函数将数据分为训练集（60%）、验证集（20%）和测试集（20%）。

### 预测列的选择
- 选择那些名称以大写字母结尾且前面都是小写字母的列为预测列`cols_pred`。

### 转换为Pandas DataFrame
- 定义了一个名为`from_polars_to_pandas`的函数，它接收一组`case_id`，过滤相应的行，并将Polars DataFrame转换为Pandas DataFrame。
- 分别获取训练集、验证集和测试集的基础信息、特征矩阵和目标变量。

### 字符串转换
- 最后对训练集、验证集和测试集中的特征矩阵调用`convert_strings`函数，该函数可能用于处理字符串类型的特征，例如编码分类变量等。

这个流程展示了如何通过一系列数据预处理步骤来准备机器学习所需的训练和测试数据集。