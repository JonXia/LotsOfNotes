{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "1c9ecfe3-d893-4b8c-9117-ce72d2ef0e2b"
   },
   "source": [
    "在上一章节，我们使用传统机器学习算法来解决了文本分类问题，从本章开始我们将尝试使用深度学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "a42981f8-6a0b-40e3-95a7-3eed6ffcff45"
   },
   "source": [
    "## **Task4 基于深度学习的文本分类1-fastText**\n",
    "\n",
    "与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。从本章开始我们将学习如何使用深度学习来完成文本表示。\n",
    "\n",
    "### **学习目标**\n",
    "\n",
    "- 学习FastText的使用和基础原理\n",
    "- 学会使用验证集进行调参\n",
    "\n",
    "### **文本表示方法 Part2**\n",
    "\n",
    "#### **现有文本表示方法的缺陷**\n",
    "\n",
    "在上一章节，我们介绍几种文本表示方法：\n",
    "\n",
    "- One-hot\n",
    "- Bag of Words\n",
    "- N-gram\n",
    "- TF-IDF\n",
    "\n",
    "也通过sklean进行了相应的实践，相信你也有了初步的认知。但上述方法都或多或少存在一定的问题：转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。\n",
    "\n",
    "与这些表示方法不同，深度学习也可以用于文本表示，还可以将其映射到一个低纬空间。其中比较典型的例子有：FastText、Word2Vec和Bert。在本章我们将介绍FastText，将在后面的内容介绍Word2Vec和Bert。\n",
    "\n",
    "#### **FastText**\n",
    "\n",
    "FastText是一种典型的深度学习词向量的表示方法，它非常简单通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。\n",
    "\n",
    "所以FastText是一个三层的神经网络，输入层、隐含层和输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "df439729-aa7a-4b45-84a0-30d0891f73e1"
   },
   "source": [
    "\n",
    "\n",
    "<div align=center><img src=\"http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279501877/1594909720411_wruLzMgC7N.jpg\" width=\"50%\" height=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "59f37db2-3255-4f2b-901c-2ba9d1596967"
   },
   "source": [
    "下图是使用keras实现的FastText网络结构："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "a74a9b4f-2a01-4f82-99f3-3649345ec645"
   },
   "source": [
    "\n",
    "<div align=center><img src=\"http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279501877/1594909821168_NvB7c98dSc.jpg\" width=\"500\" height=\"500\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "cb6130e5-442b-4534-99fb-bfa1180a9af3"
   },
   "source": [
    "FastText在文本分类任务上，是优于TF-IDF的：\n",
    "\n",
    "- FastText用单词的Embedding叠加获得的文档向量，将相似的句子分为一类\n",
    "- FastText学习到的Embedding空间维度比较低，可以快速进行训练\n",
    "\n",
    "如果想深度学习，可以参考论文：\n",
    "\n",
    "Bag of Tricks for Efficient Text Classification, https://arxiv.org/abs/1607.01759\n",
    "\n",
    "### **基于FastText的文本分类**\n",
    "\n",
    "FastText可以快速的在CPU上进行训练，最好的实践方法就是官方开源的版本：\n",
    "https://github.com/facebookresearch/fastText/tree/master/python\n",
    "\n",
    "- pip安装\n",
    "\n",
    "```\n",
    "pip install fasttext\n",
    "```\n",
    "\n",
    "- 源码安装\n",
    "\n",
    "```\n",
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "cd fastText\n",
    "sudo pip install .\n",
    "```\n",
    "\n",
    "两种安装方法都可以安装，如果你是初学者可以优先考虑使用pip安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "e76c608b-ca9d-4288-85d9-71e1c60e6c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybind11 in c:\\users\\jonxia\\appdata\\roaming\\python\\python36\\site-packages (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pybind11 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "8298f133-742a-4ac4-bbbe-3257058003bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\jonxia\\appdata\\roaming\\python\\python36\\site-packages (from fasttext) (2.5.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (from fasttext) (41.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (from fasttext) (1.16.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.16.5)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jonxia\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1321 sha256=67f14d5120ea2bd6cadbae7afef08c2828489f4e3ff05c9fa4ecb6e558069a99\n",
      "  Stored in directory: C:\\Users\\jonxia\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext --user\n",
    "# !git clone https://github.com/facebookresearch/fastText.git\n",
    "# !cd fastText && python setup.py install\n",
    "# !pip install sklearn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "uuid": "5a97d124-dcbf-48c9-8cdb-f612703c512b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8262924383461215\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "# 转换为fasttext需要的格式\n",
    "train_df = pd.read_csv('../data/train_set.csv', sep='\\t', nrows=15000)\n",
    "train_df['label_ft'] = '__label__' + train_df['label'].astype(str)\n",
    "train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')\n",
    "# 训练fasttext\n",
    "# import fasttext\n",
    "# model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2,verbose=2, minCount=1, epoch=25, loss=\"hs\")\n",
    "# val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]\n",
    "# print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "input             # training file path (required)\n",
    "lr                # learning rate [0.1]\n",
    "dim               # size of word vectors [100]\n",
    "ws                # size of the context window [5]\n",
    "epoch             # number of epochs [5]\n",
    "minCount          # minimal number of word occurences [1]\n",
    "minCountLabel     # minimal number of label occurences [1]\n",
    "minn              # min length of char ngram [0]\n",
    "maxn              # max length of char ngram [0]\n",
    "neg               # number of negatives sampled [5]\n",
    "wordNgrams        # max length of word ngram [1]\n",
    "loss              # loss function {ns, hs, softmax, ova} [softmax]\n",
    "bucket            # number of buckets [2000000]\n",
    "thread            # number of threads [number of cpus]\n",
    "lrUpdateRate      # change the rate of updates for the learning rate [100]\n",
    "t                 # sampling threshold [0.0001]\n",
    "label             # label prefix ['__label__']\n",
    "verbose           # verbose [2]\n",
    "pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('train.csv', \n",
    "                  lr=1.0, \n",
    "                  dim=200,\n",
    "                  ws=6,\n",
    "                  minCount=1,\n",
    "                  minCountLabel=1,\n",
    "                  minn=0,\n",
    "                  maxn=0,\n",
    "                  neg=5,\n",
    "                  wordNgrams=2,\n",
    "                  bucket=3000000,\n",
    "                  lrUpdateRate=10,\n",
    "                  t=0.0001,\n",
    "                  # label,\n",
    "                  # pretrainedVectors,\n",
    "                  verbose=2,\n",
    "                  epoch=25, \n",
    "                  loss=\"softmax\"\n",
    "                  )\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]\n",
    "print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "# 50000条约0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "0d2fedba-9717-4593-9cfc-c9bcee18441f"
   },
   "source": [
    "此时数据量比较小得分为0.82，当不断增加训练集数量时，FastText的精度也会不断增加5w条训练样本时，验证集得分可以到0.89-0.90左右。\n",
    "\n",
    "### **如何使用验证集调参**\n",
    "\n",
    "在使用TF-IDF和FastText中，有一些模型的参数需要选择，这些参数会在一定程度上影响模型的精度，那么如何选择这些参数呢？\n",
    "\n",
    "- 通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度\n",
    "- 通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "68a43476-af90-454f-8cb6-b8dcad3814ab"
   },
   "source": [
    "此时数据量比较小得分为0.82，当不断增加训练集数量时，FastText的精度也会不断增加5w条训练样本时，验证集得分可以到0.89-0.90左右。\n",
    "\n",
    "### **如何使用验证集调参**\n",
    "\n",
    "在使用TF-IDF和FastText中，有一些模型的参数需要选择，这些参数会在一定程度上影响模型的精度，那么如何选择这些参数呢？\n",
    "\n",
    "- 通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度\n",
    "- 通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "b7958347-4dcf-4c50-8b4f-08ad0f3bed7b"
   },
   "source": [
    "\n",
    "\n",
    "<div align=center><img src=\"http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279501877/1594909879453_RrvunJz6cT.jpg\" width=\"50%\" height=\"50%\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "63070aee-32d7-457f-9d07-5965f6ac62b9"
   },
   "source": [
    "这里我们使用10折交叉验证，每折使用9/10的数据进行训练，剩余1/10作为验证集检验模型的效果。这里需要注意每折的划分必须保证标签的分布与整个数据集的分布一致。\n",
    "\n",
    "```\n",
    "label2id = {}\n",
    "for i in range(total):\n",
    "    label = str(all_labels[i])\n",
    "    if label not in label2id:\n",
    "        label2id[label] = [i]\n",
    "    else:\n",
    "        label2id[label].append(i)\n",
    "```\n",
    "\n",
    "通过10折划分，我们一共得到了10份分布一致的数据，索引分别为0到9，每次通过将一份数据作为验证集，剩余数据作为训练集，获得了所有数据的10种分割。不失一般性，我们选择最后一份完成剩余的实验，即索引为9的一份做为验证集，索引为1-8的作为训练集，然后基于验证集的结果调整超参数，使得模型性能更优。\n",
    "\n",
    "### **本章小结**\n",
    "\n",
    "本章介绍了FastText的原理和基础使用，并进行相应的实践。然后介绍了通过10折交叉验证划分数据集。\n",
    "\n",
    "### **本章作业**\n",
    "\n",
    "- 阅读FastText的文档，尝试修改参数，得到更好的分数\n",
    "- 基于验证集的结果调整超参数，使得模型性能更优\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "8399b0eb-fd20-4c78-8f59-4ee8135fef2b"
   },
   "source": [
    "**关于Datawhale：**\n",
    "\n",
    "> Datawhale是一个专注于数据科学与AI领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。Datawhale 以“for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。同时 Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。\n",
    "\n",
    "本次新闻文本分类学习，专题知识将在天池分享，详情可关注Datawhale：\n",
    "\n",
    " ![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279172547/1584432602983_kAxAvgQpG2.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
